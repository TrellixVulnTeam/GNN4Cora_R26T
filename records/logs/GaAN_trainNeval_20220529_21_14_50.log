> Seed: 6666666
> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Initializing the Training Model: GaAN
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Constructing the Optimizer: ADAM
> Using CrossEntropyLoss as the Loss Function.

learning_rate = 0.01, epochs = 75
eval_freq = 5, optimizer = ADAM

Start Training!
------------------------------------------------------------------------
Training Round 1: loss = 2.223528, time_cost = 3.8676 sec, acc = 17.0370%
Training Round 2: loss = 1.416553, time_cost = 0.5151 sec, acc = 45.9259%
Training Round 3: loss = 0.809024, time_cost = 0.4751 sec, acc = 81.1111%
Training Round 4: loss = 0.350864, time_cost = 0.4673 sec, acc = 97.4074%
Training Round 5: loss = 0.126179, time_cost = 0.4973 sec, acc = 98.8889%
!!! Evaluation: valid_acc = 72.6601%, test_acc = 74.1082%
Training Round 6: loss = 0.040575, time_cost = 0.5413 sec, acc = 99.6296%
Training Round 7: loss = 0.013756, time_cost = 0.4940 sec, acc = 99.6296%
Training Round 8: loss = 0.006114, time_cost = 0.4738 sec, acc = 99.6296%
Training Round 9: loss = 0.006870, time_cost = 0.4903 sec, acc = 99.6296%
Training Round 10: loss = 0.002174, time_cost = 0.5201 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 77.5862%, test_acc = 79.4588%
Training Round 11: loss = 0.001611, time_cost = 0.5225 sec, acc = 100.0000%
Training Round 12: loss = 0.001822, time_cost = 0.4525 sec, acc = 100.0000%
Training Round 13: loss = 0.001653, time_cost = 0.4874 sec, acc = 100.0000%
Training Round 14: loss = 0.001147, time_cost = 0.4703 sec, acc = 100.0000%
Training Round 15: loss = 0.001320, time_cost = 0.5180 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 78.6946%, test_acc = 78.4748%
Model: model_save/20220529_21_14_50.pth has been saved since it achieves higher validation accuracy.
Training Round 16: loss = 0.001826, time_cost = 0.4814 sec, acc = 100.0000%
Training Round 17: loss = 0.002806, time_cost = 0.4652 sec, acc = 100.0000%
Training Round 18: loss = 0.004046, time_cost = 0.4918 sec, acc = 100.0000%
Training Round 19: loss = 0.006840, time_cost = 0.5150 sec, acc = 100.0000%
Training Round 20: loss = 0.011448, time_cost = 0.4801 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 80.4187%, test_acc = 80.6888%
Model: model_save/20220529_21_14_50.pth has been saved since it achieves higher validation accuracy.
Training Round 21: loss = 0.016506, time_cost = 0.5051 sec, acc = 100.0000%
Training Round 22: loss = 0.021988, time_cost = 0.4740 sec, acc = 100.0000%
Training Round 23: loss = 0.025798, time_cost = 0.4953 sec, acc = 100.0000%
Training Round 24: loss = 0.027582, time_cost = 0.4496 sec, acc = 100.0000%
Training Round 25: loss = 0.027157, time_cost = 0.4801 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 80.9113%, test_acc = 81.4268%
Model: model_save/20220529_21_14_50.pth has been saved since it achieves higher validation accuracy.
Training Round 26: loss = 0.025316, time_cost = 0.4900 sec, acc = 100.0000%
Training Round 27: loss = 0.023733, time_cost = 0.4602 sec, acc = 100.0000%
Training Round 28: loss = 0.021021, time_cost = 0.4742 sec, acc = 100.0000%
Training Round 29: loss = 0.019064, time_cost = 0.4898 sec, acc = 100.0000%
Training Round 30: loss = 0.018693, time_cost = 0.4898 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.8966%, test_acc = 83.2718%
Model: model_save/20220529_21_14_50.pth has been saved since it achieves higher validation accuracy.
Training Round 31: loss = 0.017263, time_cost = 0.4748 sec, acc = 100.0000%
Training Round 32: loss = 0.016815, time_cost = 0.4700 sec, acc = 100.0000%
Training Round 33: loss = 0.014972, time_cost = 0.4698 sec, acc = 100.0000%
Training Round 34: loss = 0.015891, time_cost = 0.4737 sec, acc = 100.0000%
Training Round 35: loss = 0.015552, time_cost = 0.4851 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 82.2660%, test_acc = 83.5178%
Model: model_save/20220529_21_14_50.pth has been saved since it achieves higher validation accuracy.
Training Round 36: loss = 0.016468, time_cost = 0.4802 sec, acc = 100.0000%
Training Round 37: loss = 0.016758, time_cost = 0.4701 sec, acc = 100.0000%
Training Round 38: loss = 0.017992, time_cost = 0.4977 sec, acc = 100.0000%
Training Round 39: loss = 0.017008, time_cost = 0.5317 sec, acc = 100.0000%
Training Round 40: loss = 0.019381, time_cost = 0.4693 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.5271%, test_acc = 83.3333%
Training Round 41: loss = 0.017996, time_cost = 0.5032 sec, acc = 100.0000%
Training Round 42: loss = 0.017529, time_cost = 0.5167 sec, acc = 100.0000%
Training Round 43: loss = 0.017114, time_cost = 0.4600 sec, acc = 100.0000%
Training Round 44: loss = 0.015992, time_cost = 0.5392 sec, acc = 100.0000%
Training Round 45: loss = 0.014252, time_cost = 0.4857 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.0345%, test_acc = 83.7638%
Training Round 46: loss = 0.014856, time_cost = 0.5680 sec, acc = 100.0000%
Training Round 47: loss = 0.015785, time_cost = 0.4858 sec, acc = 100.0000%
Training Round 48: loss = 0.014900, time_cost = 0.4891 sec, acc = 100.0000%
Training Round 49: loss = 0.014761, time_cost = 0.4499 sec, acc = 100.0000%
Training Round 50: loss = 0.015807, time_cost = 0.4933 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.2808%, test_acc = 83.8868%
Training Round 51: loss = 0.015776, time_cost = 0.4746 sec, acc = 100.0000%
Training Round 52: loss = 0.015791, time_cost = 0.5003 sec, acc = 100.0000%
Training Round 53: loss = 0.014382, time_cost = 0.4800 sec, acc = 100.0000%
Training Round 54: loss = 0.015297, time_cost = 0.4474 sec, acc = 100.0000%
Training Round 55: loss = 0.014776, time_cost = 0.4975 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.2808%, test_acc = 83.2718%
Training Round 56: loss = 0.016091, time_cost = 0.4781 sec, acc = 100.0000%
Training Round 57: loss = 0.014660, time_cost = 0.4898 sec, acc = 100.0000%
Training Round 58: loss = 0.013949, time_cost = 0.4800 sec, acc = 100.0000%
Training Round 59: loss = 0.013234, time_cost = 0.4823 sec, acc = 100.0000%
Training Round 60: loss = 0.014130, time_cost = 0.4687 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.0345%, test_acc = 83.4563%
Training Round 61: loss = 0.014111, time_cost = 0.4821 sec, acc = 100.0000%
Training Round 62: loss = 0.013373, time_cost = 0.4610 sec, acc = 100.0000%
Training Round 63: loss = 0.013558, time_cost = 0.5251 sec, acc = 100.0000%
Training Round 64: loss = 0.012842, time_cost = 0.4670 sec, acc = 100.0000%
Training Round 65: loss = 0.013460, time_cost = 0.4626 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 79.9261%, test_acc = 83.3333%
Training Round 66: loss = 0.014459, time_cost = 0.4834 sec, acc = 100.0000%
Training Round 67: loss = 0.014423, time_cost = 0.4869 sec, acc = 100.0000%
Training Round 68: loss = 0.014112, time_cost = 0.4849 sec, acc = 100.0000%
Training Round 69: loss = 0.014317, time_cost = 0.4851 sec, acc = 100.0000%
Training Round 70: loss = 0.014304, time_cost = 0.4901 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.6502%, test_acc = 83.2718%
Training Round 71: loss = 0.014176, time_cost = 0.4497 sec, acc = 100.0000%
Training Round 72: loss = 0.012743, time_cost = 0.5000 sec, acc = 100.0000%
Training Round 73: loss = 0.012442, time_cost = 0.4641 sec, acc = 100.0000%
Training Round 74: loss = 0.013636, time_cost = 0.5376 sec, acc = 100.0000%
Training Round 75: loss = 0.012804, time_cost = 0.4549 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.6502%, test_acc = 83.0873%
> Training finished.

> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Loading model_save/20220529_21_14_50.pth
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Evaluation Results: valid_acc = 81.8966%, test_acc = 83.3948%
> Evaluation finished.
