> Seed: 6666666
> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Initializing the Training Model: GAT
> Model Structure:
GAT(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Constructing the Optimizer: ADAM
> Using CrossEntropyLoss as the Loss Function.

learning_rate = 0.01, epochs = 75
eval_freq = 5, optimizer = ADAM

Start Training!
------------------------------------------------------------------------
Training Round 1: loss = 2.692621, time_cost = 1.6942 sec, acc = 10.0000%
Training Round 2: loss = 2.021471, time_cost = 0.2809 sec, acc = 33.7037%
Training Round 3: loss = 3.091064, time_cost = 0.2381 sec, acc = 34.8148%
Training Round 4: loss = 1.346494, time_cost = 0.2421 sec, acc = 57.4074%
Training Round 5: loss = 0.626421, time_cost = 0.2552 sec, acc = 82.2222%
!!! Evaluation: valid_acc = 61.8227%, test_acc = 61.1931%
Training Round 6: loss = 0.535648, time_cost = 0.2434 sec, acc = 79.6296%
Training Round 7: loss = 0.264977, time_cost = 0.2332 sec, acc = 94.4444%
Training Round 8: loss = 0.171434, time_cost = 0.2372 sec, acc = 96.6667%
Training Round 9: loss = 0.118392, time_cost = 0.2460 sec, acc = 98.1481%
Training Round 10: loss = 0.078329, time_cost = 0.2347 sec, acc = 98.5185%
!!! Evaluation: valid_acc = 77.0936%, test_acc = 79.3973%
Training Round 11: loss = 0.052847, time_cost = 0.2335 sec, acc = 98.8889%
Training Round 12: loss = 0.031484, time_cost = 0.2977 sec, acc = 99.2593%
Training Round 13: loss = 0.021119, time_cost = 0.2438 sec, acc = 99.6296%
Training Round 14: loss = 0.016039, time_cost = 0.2349 sec, acc = 99.6296%
Training Round 15: loss = 0.010942, time_cost = 0.2309 sec, acc = 99.6296%
!!! Evaluation: valid_acc = 79.8030%, test_acc = 81.7343%
Model: model_save/20220527_21_54_18.pth has been saved since it achieves higher validation accuracy.
Training Round 16: loss = 0.009228, time_cost = 0.2438 sec, acc = 99.6296%
Training Round 17: loss = 0.007194, time_cost = 0.2528 sec, acc = 100.0000%
Training Round 18: loss = 0.005380, time_cost = 0.2573 sec, acc = 100.0000%
Training Round 19: loss = 0.003505, time_cost = 0.2822 sec, acc = 100.0000%
Training Round 20: loss = 0.003091, time_cost = 0.2651 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 80.9113%, test_acc = 82.2263%
Model: model_save/20220527_21_54_18.pth has been saved since it achieves higher validation accuracy.
Training Round 21: loss = 0.002818, time_cost = 0.2807 sec, acc = 100.0000%
Training Round 22: loss = 0.002624, time_cost = 0.2628 sec, acc = 100.0000%
Training Round 23: loss = 0.002907, time_cost = 0.2820 sec, acc = 100.0000%
Training Round 24: loss = 0.003313, time_cost = 0.2752 sec, acc = 100.0000%
Training Round 25: loss = 0.004396, time_cost = 0.2807 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 80.6650%, test_acc = 81.9803%
Training Round 26: loss = 0.005589, time_cost = 0.2772 sec, acc = 100.0000%
Training Round 27: loss = 0.006672, time_cost = 0.2706 sec, acc = 100.0000%
Training Round 28: loss = 0.007514, time_cost = 0.2783 sec, acc = 100.0000%
Training Round 29: loss = 0.008941, time_cost = 0.2866 sec, acc = 100.0000%
Training Round 30: loss = 0.010172, time_cost = 0.2786 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.7734%, test_acc = 82.2878%
Model: model_save/20220527_21_54_18.pth has been saved since it achieves higher validation accuracy.
Training Round 31: loss = 0.011458, time_cost = 0.2893 sec, acc = 100.0000%
Training Round 32: loss = 0.013107, time_cost = 0.2721 sec, acc = 100.0000%
Training Round 33: loss = 0.013908, time_cost = 0.2650 sec, acc = 100.0000%
Training Round 34: loss = 0.015968, time_cost = 0.2691 sec, acc = 100.0000%
Training Round 35: loss = 0.016559, time_cost = 0.2886 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.1576%, test_acc = 83.2103%
Training Round 36: loss = 0.016885, time_cost = 0.2821 sec, acc = 100.0000%
Training Round 37: loss = 0.017203, time_cost = 0.2838 sec, acc = 100.0000%
Training Round 38: loss = 0.016778, time_cost = 0.2709 sec, acc = 100.0000%
Training Round 39: loss = 0.016532, time_cost = 0.2918 sec, acc = 100.0000%
Training Round 40: loss = 0.019204, time_cost = 0.2727 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.8966%, test_acc = 83.5178%
Model: model_save/20220527_21_54_18.pth has been saved since it achieves higher validation accuracy.
Training Round 41: loss = 0.017134, time_cost = 0.2816 sec, acc = 100.0000%
Training Round 42: loss = 0.016969, time_cost = 0.2669 sec, acc = 100.0000%
Training Round 43: loss = 0.016296, time_cost = 0.2757 sec, acc = 100.0000%
Training Round 44: loss = 0.015802, time_cost = 0.2660 sec, acc = 100.0000%
Training Round 45: loss = 0.014054, time_cost = 0.2679 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.7734%, test_acc = 82.9028%
Training Round 46: loss = 0.014965, time_cost = 0.2851 sec, acc = 100.0000%
Training Round 47: loss = 0.016206, time_cost = 0.2810 sec, acc = 100.0000%
Training Round 48: loss = 0.014802, time_cost = 0.2642 sec, acc = 100.0000%
Training Round 49: loss = 0.014580, time_cost = 0.2806 sec, acc = 100.0000%
Training Round 50: loss = 0.015009, time_cost = 0.2732 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.8966%, test_acc = 83.1488%
Training Round 51: loss = 0.015727, time_cost = 0.2848 sec, acc = 100.0000%
Training Round 52: loss = 0.015154, time_cost = 0.2861 sec, acc = 100.0000%
Training Round 53: loss = 0.013684, time_cost = 0.2727 sec, acc = 100.0000%
Training Round 54: loss = 0.014575, time_cost = 0.2767 sec, acc = 100.0000%
Training Round 55: loss = 0.014633, time_cost = 0.2748 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 82.1429%, test_acc = 82.9643%
Model: model_save/20220527_21_54_18.pth has been saved since it achieves higher validation accuracy.
Training Round 56: loss = 0.015963, time_cost = 0.2766 sec, acc = 100.0000%
Training Round 57: loss = 0.015886, time_cost = 0.2939 sec, acc = 100.0000%
Training Round 58: loss = 0.015365, time_cost = 0.2761 sec, acc = 100.0000%
Training Round 59: loss = 0.014535, time_cost = 0.2787 sec, acc = 100.0000%
Training Round 60: loss = 0.015466, time_cost = 0.2710 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.2808%, test_acc = 83.1488%
Training Round 61: loss = 0.015946, time_cost = 0.2727 sec, acc = 100.0000%
Training Round 62: loss = 0.015091, time_cost = 0.2716 sec, acc = 100.0000%
Training Round 63: loss = 0.015010, time_cost = 0.2829 sec, acc = 100.0000%
Training Round 64: loss = 0.014123, time_cost = 0.2760 sec, acc = 100.0000%
Training Round 65: loss = 0.014377, time_cost = 0.2726 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.5271%, test_acc = 83.3333%
Training Round 66: loss = 0.015361, time_cost = 0.2742 sec, acc = 100.0000%
Training Round 67: loss = 0.015189, time_cost = 0.2934 sec, acc = 100.0000%
Training Round 68: loss = 0.014812, time_cost = 0.2902 sec, acc = 100.0000%
Training Round 69: loss = 0.015351, time_cost = 0.2838 sec, acc = 100.0000%
Training Round 70: loss = 0.015540, time_cost = 0.2598 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 81.0345%, test_acc = 83.1488%
Training Round 71: loss = 0.016236, time_cost = 0.2744 sec, acc = 100.0000%
Training Round 72: loss = 0.014640, time_cost = 0.2651 sec, acc = 100.0000%
Training Round 73: loss = 0.014785, time_cost = 0.2846 sec, acc = 100.0000%
Training Round 74: loss = 0.016128, time_cost = 0.2560 sec, acc = 100.0000%
Training Round 75: loss = 0.014840, time_cost = 0.2628 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 82.0197%, test_acc = 82.9643%
> Training finished.

> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Loading model_save/20220527_21_54_18.pth
> Model Structure:
GAT(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Evaluation Results: valid_acc = 82.0197%, test_acc = 83.0873%
> Evaluation finished.
