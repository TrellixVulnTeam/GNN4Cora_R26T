> Seed: 6666666
> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 140, validation = 500, test = 1000
> train_set_imbalance: {0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 20}
> Initializing the Training Model: GaAN
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Constructing the Optimizer: ADAM
> Using CrossEntropyLoss as the Loss Function.

learning_rate = 0.01, epochs = 75
eval_freq = 5, optimizer = ADAM

Start Training!
------------------------------------------------------------------------
Training Round 1: loss = 2.124633, time_cost = 2.0776 sec, acc = 18.5714%
Training Round 2: loss = 1.220855, time_cost = 0.4202 sec, acc = 72.1429%
Training Round 3: loss = 0.483883, time_cost = 0.4601 sec, acc = 95.0000%
Training Round 4: loss = 0.106009, time_cost = 0.4298 sec, acc = 98.5714%
Training Round 5: loss = 0.021327, time_cost = 0.4406 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 67.4000%, test_acc = 70.7000%
Training Round 6: loss = 0.017360, time_cost = 0.4229 sec, acc = 100.0000%
Training Round 7: loss = 0.002258, time_cost = 0.4503 sec, acc = 100.0000%
Training Round 8: loss = 0.000896, time_cost = 0.4760 sec, acc = 100.0000%
Training Round 9: loss = 0.000937, time_cost = 0.4100 sec, acc = 100.0000%
Training Round 10: loss = 0.000423, time_cost = 0.4304 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 68.8000%, test_acc = 68.4000%
Training Round 11: loss = 0.000331, time_cost = 0.4625 sec, acc = 100.0000%
Training Round 12: loss = 0.000387, time_cost = 0.4272 sec, acc = 100.0000%
Training Round 13: loss = 0.000510, time_cost = 0.4201 sec, acc = 100.0000%
Training Round 14: loss = 0.000844, time_cost = 0.4098 sec, acc = 100.0000%
Training Round 15: loss = 0.001134, time_cost = 0.4176 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 70.0000%, test_acc = 69.6000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 16: loss = 0.001595, time_cost = 0.3998 sec, acc = 100.0000%
Training Round 17: loss = 0.003154, time_cost = 0.4196 sec, acc = 100.0000%
Training Round 18: loss = 0.004918, time_cost = 0.4201 sec, acc = 100.0000%
Training Round 19: loss = 0.007268, time_cost = 0.4202 sec, acc = 100.0000%
Training Round 20: loss = 0.011284, time_cost = 0.4197 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 72.8000%, test_acc = 75.3000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 21: loss = 0.018203, time_cost = 0.4207 sec, acc = 100.0000%
Training Round 22: loss = 0.022120, time_cost = 0.4303 sec, acc = 100.0000%
Training Round 23: loss = 0.028689, time_cost = 0.4402 sec, acc = 100.0000%
Training Round 24: loss = 0.030147, time_cost = 0.4403 sec, acc = 100.0000%
Training Round 25: loss = 0.028489, time_cost = 0.4309 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 76.4000%, test_acc = 75.9000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 26: loss = 0.024724, time_cost = 0.4202 sec, acc = 100.0000%
Training Round 27: loss = 0.019174, time_cost = 0.4246 sec, acc = 100.0000%
Training Round 28: loss = 0.015274, time_cost = 0.4050 sec, acc = 100.0000%
Training Round 29: loss = 0.013691, time_cost = 0.4602 sec, acc = 100.0000%
Training Round 30: loss = 0.012905, time_cost = 0.4449 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 77.0000%, test_acc = 78.1000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 31: loss = 0.012330, time_cost = 0.4183 sec, acc = 100.0000%
Training Round 32: loss = 0.012676, time_cost = 0.4399 sec, acc = 100.0000%
Training Round 33: loss = 0.012162, time_cost = 0.4186 sec, acc = 100.0000%
Training Round 34: loss = 0.012201, time_cost = 0.4396 sec, acc = 100.0000%
Training Round 35: loss = 0.013286, time_cost = 0.4211 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 77.2000%, test_acc = 78.0000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 36: loss = 0.014946, time_cost = 0.4200 sec, acc = 100.0000%
Training Round 37: loss = 0.016407, time_cost = 0.4435 sec, acc = 100.0000%
Training Round 38: loss = 0.016366, time_cost = 0.4262 sec, acc = 100.0000%
Training Round 39: loss = 0.015105, time_cost = 0.4412 sec, acc = 100.0000%
Training Round 40: loss = 0.016410, time_cost = 0.4285 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 77.2000%, test_acc = 78.7000%
Training Round 41: loss = 0.015506, time_cost = 0.4100 sec, acc = 100.0000%
Training Round 42: loss = 0.015578, time_cost = 0.4104 sec, acc = 100.0000%
Training Round 43: loss = 0.014306, time_cost = 0.4578 sec, acc = 100.0000%
Training Round 44: loss = 0.012657, time_cost = 0.4322 sec, acc = 100.0000%
Training Round 45: loss = 0.010944, time_cost = 0.4200 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 79.6000%, test_acc = 78.6000%
Model: model_save/20220530_03_27_14.pth has been saved since it achieves higher validation accuracy.
Training Round 46: loss = 0.011902, time_cost = 0.4398 sec, acc = 100.0000%
Training Round 47: loss = 0.012725, time_cost = 0.4500 sec, acc = 100.0000%
Training Round 48: loss = 0.012532, time_cost = 0.4498 sec, acc = 100.0000%
Training Round 49: loss = 0.012560, time_cost = 0.4104 sec, acc = 100.0000%
Training Round 50: loss = 0.015084, time_cost = 0.4456 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 79.2000%, test_acc = 78.9000%
Training Round 51: loss = 0.014811, time_cost = 0.4332 sec, acc = 100.0000%
Training Round 52: loss = 0.014181, time_cost = 0.4348 sec, acc = 100.0000%
Training Round 53: loss = 0.012339, time_cost = 0.4153 sec, acc = 100.0000%
Training Round 54: loss = 0.013112, time_cost = 0.4336 sec, acc = 100.0000%
Training Round 55: loss = 0.011957, time_cost = 0.4263 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 78.2000%, test_acc = 78.2000%
Training Round 56: loss = 0.014020, time_cost = 0.4100 sec, acc = 100.0000%
Training Round 57: loss = 0.011838, time_cost = 0.4202 sec, acc = 100.0000%
Training Round 58: loss = 0.012052, time_cost = 0.4148 sec, acc = 100.0000%
Training Round 59: loss = 0.011315, time_cost = 0.4127 sec, acc = 100.0000%
Training Round 60: loss = 0.012959, time_cost = 0.4223 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 78.8000%, test_acc = 78.6000%
Training Round 61: loss = 0.012686, time_cost = 0.4268 sec, acc = 100.0000%
Training Round 62: loss = 0.012045, time_cost = 0.4325 sec, acc = 100.0000%
Training Round 63: loss = 0.011568, time_cost = 0.4377 sec, acc = 100.0000%
Training Round 64: loss = 0.011057, time_cost = 0.5262 sec, acc = 100.0000%
Training Round 65: loss = 0.011230, time_cost = 0.4757 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 77.2000%, test_acc = 77.3000%
Training Round 66: loss = 0.012592, time_cost = 0.4563 sec, acc = 100.0000%
Training Round 67: loss = 0.012891, time_cost = 0.4854 sec, acc = 100.0000%
Training Round 68: loss = 0.012624, time_cost = 0.4744 sec, acc = 100.0000%
Training Round 69: loss = 0.012878, time_cost = 0.5246 sec, acc = 100.0000%
Training Round 70: loss = 0.012280, time_cost = 0.4810 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 78.0000%, test_acc = 77.7000%
Training Round 71: loss = 0.012474, time_cost = 0.4804 sec, acc = 100.0000%
Training Round 72: loss = 0.011080, time_cost = 0.4546 sec, acc = 100.0000%
Training Round 73: loss = 0.010781, time_cost = 0.4474 sec, acc = 100.0000%
Training Round 74: loss = 0.011519, time_cost = 0.4719 sec, acc = 100.0000%
Training Round 75: loss = 0.011337, time_cost = 0.4862 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 78.4000%, test_acc = 77.4000%
> Training finished.

> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: both
> num_nodes: 2708, num_edges: [10556]
> num_feats: 1433, num_classes: 7
> num_samples: training = 140, validation = 500, test = 1000
> train_set_imbalance: {0: 20, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 20}
> Loading model_save/20220530_03_27_14.pth
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Evaluation Results: valid_acc = 77.8000%, test_acc = 79.0000%
> Evaluation finished.
