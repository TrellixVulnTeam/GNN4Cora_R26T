> Seed: 6666666
> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: cited
> num_nodes: 2708, num_edges: [5429]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Initializing the Training Model: GaAN
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Constructing the Optimizer: ADAM
> Using CrossEntropyLoss as the Loss Function.

learning_rate = 0.01, epochs = 75
eval_freq = 5, optimizer = ADAM

Start Training!
------------------------------------------------------------------------
Training Round 1: loss = 2.169399, time_cost = 1.8642 sec, acc = 16.2963%
Training Round 2: loss = 1.338683, time_cost = 0.4440 sec, acc = 51.8519%
Training Round 3: loss = 0.755841, time_cost = 0.4396 sec, acc = 83.3333%
Training Round 4: loss = 0.356798, time_cost = 0.4098 sec, acc = 97.0370%
Training Round 5: loss = 0.158768, time_cost = 0.4103 sec, acc = 98.5185%
!!! Evaluation: valid_acc = 66.1330%, test_acc = 68.2657%
Training Round 6: loss = 0.073811, time_cost = 0.4196 sec, acc = 99.6296%
Training Round 7: loss = 0.036010, time_cost = 0.4000 sec, acc = 99.6296%
Training Round 8: loss = 0.021007, time_cost = 0.4623 sec, acc = 100.0000%
Training Round 9: loss = 0.012851, time_cost = 0.3988 sec, acc = 100.0000%
Training Round 10: loss = 0.011332, time_cost = 0.4589 sec, acc = 99.6296%
!!! Evaluation: valid_acc = 70.0739%, test_acc = 72.6937%
Training Round 11: loss = 0.006142, time_cost = 0.4203 sec, acc = 100.0000%
Training Round 12: loss = 0.005017, time_cost = 0.4598 sec, acc = 100.0000%
Training Round 13: loss = 0.005140, time_cost = 0.4020 sec, acc = 100.0000%
Training Round 14: loss = 0.005716, time_cost = 0.4181 sec, acc = 100.0000%
Training Round 15: loss = 0.007175, time_cost = 0.4097 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 69.2118%, test_acc = 72.6322%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 16: loss = 0.008464, time_cost = 0.4086 sec, acc = 100.0000%
Training Round 17: loss = 0.010464, time_cost = 0.3996 sec, acc = 100.0000%
Training Round 18: loss = 0.015779, time_cost = 0.4500 sec, acc = 100.0000%
Training Round 19: loss = 0.023481, time_cost = 0.4401 sec, acc = 100.0000%
Training Round 20: loss = 0.030106, time_cost = 0.4180 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 69.5813%, test_acc = 73.0012%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 21: loss = 0.036956, time_cost = 0.4514 sec, acc = 100.0000%
Training Round 22: loss = 0.042967, time_cost = 0.4085 sec, acc = 100.0000%
Training Round 23: loss = 0.044518, time_cost = 0.4452 sec, acc = 100.0000%
Training Round 24: loss = 0.043549, time_cost = 0.4102 sec, acc = 100.0000%
Training Round 25: loss = 0.040552, time_cost = 0.4201 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 69.7044%, test_acc = 72.5707%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 26: loss = 0.037450, time_cost = 0.4604 sec, acc = 100.0000%
Training Round 27: loss = 0.033201, time_cost = 0.4309 sec, acc = 100.0000%
Training Round 28: loss = 0.030141, time_cost = 0.4586 sec, acc = 100.0000%
Training Round 29: loss = 0.028123, time_cost = 0.4400 sec, acc = 100.0000%
Training Round 30: loss = 0.025363, time_cost = 0.4260 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 69.8276%, test_acc = 73.0012%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 31: loss = 0.024054, time_cost = 0.4401 sec, acc = 100.0000%
Training Round 32: loss = 0.023389, time_cost = 0.4448 sec, acc = 100.0000%
Training Round 33: loss = 0.022885, time_cost = 0.4178 sec, acc = 100.0000%
Training Round 34: loss = 0.024452, time_cost = 0.4125 sec, acc = 100.0000%
Training Round 35: loss = 0.024358, time_cost = 0.4247 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 69.9507%, test_acc = 72.6322%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 36: loss = 0.026156, time_cost = 0.4200 sec, acc = 100.0000%
Training Round 37: loss = 0.026225, time_cost = 0.4206 sec, acc = 100.0000%
Training Round 38: loss = 0.026531, time_cost = 0.4149 sec, acc = 100.0000%
Training Round 39: loss = 0.027111, time_cost = 0.4401 sec, acc = 100.0000%
Training Round 40: loss = 0.027585, time_cost = 0.4431 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 70.8128%, test_acc = 72.6322%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 41: loss = 0.026025, time_cost = 0.5500 sec, acc = 100.0000%
Training Round 42: loss = 0.025976, time_cost = 0.4495 sec, acc = 100.0000%
Training Round 43: loss = 0.025554, time_cost = 0.5038 sec, acc = 100.0000%
Training Round 44: loss = 0.024395, time_cost = 0.4471 sec, acc = 100.0000%
Training Round 45: loss = 0.023166, time_cost = 0.4589 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 70.5665%, test_acc = 72.6322%
Training Round 46: loss = 0.022839, time_cost = 0.4548 sec, acc = 100.0000%
Training Round 47: loss = 0.022161, time_cost = 0.4480 sec, acc = 100.0000%
Training Round 48: loss = 0.021695, time_cost = 0.4608 sec, acc = 100.0000%
Training Round 49: loss = 0.022412, time_cost = 0.4467 sec, acc = 100.0000%
Training Round 50: loss = 0.021935, time_cost = 0.4626 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 71.4286%, test_acc = 73.6777%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 51: loss = 0.022633, time_cost = 0.4761 sec, acc = 100.0000%
Training Round 52: loss = 0.023071, time_cost = 0.4725 sec, acc = 100.0000%
Training Round 53: loss = 0.023136, time_cost = 0.4704 sec, acc = 100.0000%
Training Round 54: loss = 0.022956, time_cost = 0.4811 sec, acc = 100.0000%
Training Round 55: loss = 0.023332, time_cost = 0.4690 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 71.6749%, test_acc = 73.3702%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 56: loss = 0.022167, time_cost = 0.4647 sec, acc = 100.0000%
Training Round 57: loss = 0.022633, time_cost = 0.4700 sec, acc = 100.0000%
Training Round 58: loss = 0.021951, time_cost = 0.4351 sec, acc = 100.0000%
Training Round 59: loss = 0.021927, time_cost = 0.4741 sec, acc = 100.0000%
Training Round 60: loss = 0.021677, time_cost = 0.4523 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 71.5517%, test_acc = 73.5547%
Training Round 61: loss = 0.021289, time_cost = 0.5577 sec, acc = 100.0000%
Training Round 62: loss = 0.020832, time_cost = 0.4952 sec, acc = 100.0000%
Training Round 63: loss = 0.021227, time_cost = 0.4735 sec, acc = 100.0000%
Training Round 64: loss = 0.020965, time_cost = 0.5066 sec, acc = 100.0000%
Training Round 65: loss = 0.021709, time_cost = 0.4496 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 70.9360%, test_acc = 73.3702%
Training Round 66: loss = 0.021161, time_cost = 0.5102 sec, acc = 100.0000%
Training Round 67: loss = 0.021299, time_cost = 0.4448 sec, acc = 100.0000%
Training Round 68: loss = 0.021121, time_cost = 0.4852 sec, acc = 100.0000%
Training Round 69: loss = 0.021234, time_cost = 0.4802 sec, acc = 100.0000%
Training Round 70: loss = 0.022114, time_cost = 0.4398 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 71.9212%, test_acc = 73.8007%
Model: model_save/20220530_03_40_09.pth has been saved since it achieves higher validation accuracy.
Training Round 71: loss = 0.020641, time_cost = 0.4701 sec, acc = 100.0000%
Training Round 72: loss = 0.021481, time_cost = 0.4473 sec, acc = 100.0000%
Training Round 73: loss = 0.019995, time_cost = 0.4618 sec, acc = 100.0000%
Training Round 74: loss = 0.020371, time_cost = 0.4800 sec, acc = 100.0000%
Training Round 75: loss = 0.020786, time_cost = 0.4598 sec, acc = 100.0000%
!!! Evaluation: valid_acc = 71.5517%, test_acc = 73.6162%
> Training finished.

> device: cuda:0
> Loading DataSet from data/cora/
> Data sent to cuda:0
> view: cited
> num_nodes: 2708, num_edges: [5429]
> num_feats: 1433, num_classes: 7
> num_samples: training = 270, validation = 812, test = 1626
> train_set_imbalance: {0: 28, 1: 35, 2: 79, 3: 50, 4: 22, 5: 20, 6: 36}
> Loading model_save/20220530_03_40_09.pth
> Model Structure:
GaAN(
  (proj_fc): Linear(in_features=1433, out_features=128, bias=False)
  (embed_fc): Linear(in_features=128, out_features=128, bias=False)
  (layers): ModuleList(
    (0): ModuleList(
      (0): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
      (1): MultiHeadCGaANLayer(
        (cGaANs): ModuleList(
          (0): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (1): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
          (2): CGaANLayer(
            (Wa): Linear(in_features=128, out_features=128, bias=False)
            (att_out_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (att_out_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_l): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_m): Linear(in_features=128, out_features=1, bias=False)
            (gate_fc_r): Linear(in_features=128, out_features=1, bias=False)
            (Wgm): Linear(in_features=128, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (tran_fc): Linear(in_features=512, out_features=7, bias=True)
)
> Model sent to cuda:0
> Evaluation Results: valid_acc = 71.5517%, test_acc = 73.6777%
> Evaluation finished.
